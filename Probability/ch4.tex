\setcounter{chapter}{3}
\chapter{Random Variables}
In tossing dice, we are more frequently interested in the sum of the two dice rather than the separate outcomes of each die. In flipping a coin, we often look at the head-tail sequence that follows, not just the individual heads or tails outcomes. We defined these real valued functions on the sample space as \textbf{random variables}.

\subsection*{Example}
Suppose that our experiment consists of tossing 3 fair coins. If we let denote $Y$ the number of heads that appear, then $Y$ is a random variable taking on one of the values 0, 1, 2, and 3 with respective probabilities
\begin{equation*}
    \begin{split}
        P\{Y = 0\} &= P\{(t,t,t)\} = \frac{1}{8}\\
        P\{Y = 1\} &= P\{(t,t,h),(t,h,t),(h,t,t)\} = \frac{3}{8}\\
        P\{Y = 2\} &= P\{(t,h,h), (h,t,h), (h,h,t)\} = \frac{3}{8}\\
        P\{Y = 3\} &= P\{(h,h,h)\} = \frac{1}{8}\\
    \end{split}
\end{equation*}
$Y$ takes on one of the values 0 through 3, so we can write it as 
\[1 = P)\bigcup^3_{i=1} \{Y = i\} = \sum^3_{i=0} P\{Y = i\}\]
\section{Discrete Random Variables}
A random variable that can take on at most a countable number of possible values is considered \textbf{discrete}.
\begin{definition}[Probability Mass Function (PMF)]
For a discrete random variable, we define the \textbf{probability mass function} as the probability that the discrete random variable is equal to a certain value.
\[p(a) = P\{X = a\}\]
\end{definition}
\begin{definition}[Cumulative Distribution Function]
    The CDF can be expressed in terms of $p(a)$ as \[F(a) = \sum_{\text{all} x\leq a} p(x)\]
    If x is a discrete random variable where possible values are $x_1,x_2,x_3,\dots$ where $x_1 < x_2 < x < 3 < \cdots$, then the distribution function $F$ of $X$ is a step function. 
\end{definition}
\section{Expected Value}
\begin{definition}[Expected Value]
    The expected value of $X$ is a weighted average of the possible values that $X$ can take on, each value weighted by the probability that $X$ assigns it. 
    \[E[X] = \sum_{x:p(x) > 0} xp(x)\]
\end{definition}
\subsection*{Example}
Find $E[X]$, where $X$ is the outcome when we roll a fair die. \\
Since $p(1) = p(2) = p(3) = p(4) = p(5) = p(6) = \frac{1}{6}$, we have 
\[E[X] = 1\left(\frac{1}{6}\right) + 2\left(\frac{1}{6}\right) + 3\left(\frac{1}{6}\right) + 4\left(\frac{1}{6}\right) + 5\left(\frac{1}{6}\right) + 6\left(\frac{1}{6}\right) = \frac{7}{2}\]
\subsection*{Example}
We say that $I$ is an indicator variable for the event $A$ if 
\begin{equation}
I = 
\left\{
    \begin{array}{lr}
        1, & \text{if } A \text{ occurs}\\
        0, & \text{if } A^c \text{ occurs}
    \end{array}
\right\}
\end{equation}
Find $E[I]$.\\
Since $p(1) = p(A), p(0) = 1 - P(A)$, \[E[I] = P(A)\]
\section{Expectation of a Function of a Random Variable}
\subsection*{Example}
Let $X$ denote a r.v. that takes on any of the values $-1,0,1$ with respective probabilities 
\[P\{X = -1\} = .2\qquad P\{X = 0\} = .5\qquad P\{X = 1\} = .3\]
Compute $E[X^2]$.\\
Let $Y = X^2$. Then the probability mass function of $Y$ is given by 
\begin{equation*}
    \begin{split}
        P\{Y = 1\} = P\{X = -1\} + P\{X = 1\} = .5
        P\{Y = 0\} = P\{X = 0\} = .5
    \end{split}
\end{equation*}
Then, \[E[X^2] = E[Y] = 1(.5) + 0(.5) = .5\]
\textbf{Note that}:
\[.5 = E[X^2]\neq E[X]^2 = .01\]
Take $g(x)$ to be a probability mass function. $E[g(x)]$ can be seen as the weighted average of the values $g(x)$, with $g(x)$ being weighted by the probability that $X = x$
\subsubsection*{Proposition 4.1}
If $X$ is a discrete r.v. that takes on one of the values $x_i, i\geq 1$, with respective probabilities $p(x_i)$, then for any real-valued function $g$, \[E[g(X)] = \sum_i g(x_i)p(x_i)\]
\section{Variance}
\begin{definition}[Variance]
    Variance is considered to be the spread of the values around $X$. If $X$ is a r.v. with mean $\mu$, then the \textbf{variance} of $X$, denoted by $Var(X)$, is defined as 
    \[Var(x) = E[(X-\mu)^2]\]
    An alternative formula can be defined as:
    \begin{equation*}
        \begin{split}
            Var(X) &= E[(X-\mu)^2]\\
            &= \sum_x (x-\mu)^2p(x)\\
            &= \sum_x (x^2 - 2\mu x + \mu^2)p(x)\\
            &= \sum_x x^2p(x) - 2\mu\sum_x xp(x) + \mu^2 \sum_x p(x)\\
            &= E[X^2] - 2\mu^2 + \mu^2\\
            &= E[X^2] - \mu^2
        \end{split}
    \end{equation*}
    So we have:
    \[Var(X) = E[X^2] - (E[X])^2\]
\end{definition}
\subsection*{Example}
Calculate $Var(X)$ if $X$ represents the outcome when a fair die is rolled. \\
We have shown that $E[X] = \frac{7}{2}$
\[E[X^2] = 1^2\left(\frac{1}{6}\right) + 2^2\left(\frac{1}{6}\right) + 3^2\left(\frac{1}{6}\right) + 4^2\left(\frac{1}{6}\right) + 5^2\left(\frac{1}{6}\right) + 6^2\left(\frac{1}{6}\right) = \left(\frac{1}{6}\right)(91)\]
Then, \[Var(X) = \frac{91}{6} - \left(\frac{7}{2}\right)^2 = \boxed{\frac{35}{12}}\]
Because $Var(X) = E[(X-\mu)^2] = \sum_x (x-\mu)^2 P(X = x)$ is the sum of nonnegative terms, it follows that $Var(X)\geq 0$ and that \[E[X^2]\geq (E[X])^2\]
\section{Bernoulli and Binomial Random Variables}
For a trial/experiment, its outcomes can be classified as either a \textit{success} or a \textit{failure}. If we assign $X=1$ for a success and $X = 0$ for a failure, then the probability mass function (PMF) of $X$ is
\[p(0) = P\{X = 0\} = 1 - p\] \[p(1) = P\{X = 1\} = p\]
\begin{definition}[Bernoulli Random Variable]
    A random variable X is said to be a \textbf{Bernoulli random variable} if its probability mass function takes on values for some $p\in(0,1)$ with parameters $(1,p)$
\end{definition}
\begin{definition}[Binomial Random Variable]
    Suppose now we look at $n$ independent trails, with the same probability assignments. If $X$ represents the numbers of successes that occur in the $n$ trials, then $X$ is said to be a \textbf{Binomial random variable} with parameters $(n,p)$. The PMF is defined as \[p(i) = {n\choose i}p^i(1-p)^{n-i}\qquad i = 0,1,\dots, n\]
\end{definition}
\subsection*{Example}
Five fair coins are flipped. If the outcomes are assumed independent, find the probability mass function of the number of heads obtained.\\

If we let $X$ equal the number of heads (successes) that appear, then $X$ is a binomial r.v. with parameters $n = 5, p = \frac{1}{2}$. Defining the PMF, we have
\[P\{X = 0\} = {5\choose 0}\left(\frac{1}{2}\right)^0\left(\frac{1}{2}\right)^5 = \frac{1}{32}\]
\[P\{X = 1\} = {5\choose 1}\left(\frac{1}{2}\right)^1\left(\frac{1}{2}\right)^4 = \frac{5}{32}\]
\[P\{X = 2\} = {5\choose 2}\left(\frac{1}{2}\right)^2\left(\frac{1}{2}\right)^3 = \frac{10}{32}\]
\[P\{X = 3\} = {5\choose 3}\left(\frac{1}{2}\right)^3\left(\frac{1}{2}\right)^2 = \frac{10}{32}\]
\[P\{X = 4\} = {5\choose 4}\left(\frac{1}{2}\right)^4\left(\frac{1}{2}\right)^1 = \frac{5}{32}\]
\[P\{X = 5\} = {5\choose 5}\left(\frac{1}{2}\right)^5\left(\frac{1}{2}\right)^0 = \frac{1}{32}\]

\subsection*{Example}
It is known that screws produced by a certain company will be defective with probability .01, independently of one another. The company sells the screws in packages of 10 and offers a money-back guarantee that at most 1 of the 10
screws is defective. What proportion of packages sold must the company replace?\\
If $X$ is the number of defective screws in a package, then $X$ is a binomial random variable with parameters $(10, .01)$. The probability that a package has to be replaced is
\[1 - P\{X = 0\} - P\{X = 1\} = 1 - {10\choose 0}(.01)^0(.99)^10 - {10\choose 1}(.01)^1(.99)^9\]
\[\approx .004\]
Only $.4\%$ of the packages need to be replaced. 
\subsection{Properties of Binomial Random Variables}
Compute the Expected Value and Variance.
\begin{equation*}
    \begin{split}
        E[X^k] = \sum^n_{i = 0} i^k{n\choose i}p^i(1-p)^n-i
    \end{split}
\end{equation*}
Using the identity \[i{n\choose i} = n{(n-1)\choose (i-1)}\]
gives 
\begin{equation*}
    \begin{split}
        E[X^k] &= np \sum^n_{i=1} i^{k - 1}{(n-1)\choose (i-1)}p^{i-1}(1-p)^{n-i}\\
        &= np\sum^{n-1}_{j = 0} (j+1)^{k-1}{(n-1)\choose j}p^j(1-p)^{n-1-j}\qquad \text{by letting } j = i-1\\
        &=  npE[(Y+1)^{k-1}]
    \end{split}
\end{equation*}
Setting $k = 1$ in the preceding equation yeilds $$E[X] = np$$
Setting $k = 2$ and using the formula for the expected value of a binomial variable yields
\begin{equation*}
    \begin{split}
        E[X^2] &= npE[Y+1]\\
        &= np[(n-1)p + 1]
    \end{split}
\end{equation*}
With $E[X] = np$, we have 
\begin{equation*}
    \begin{split}
        Var(X) &= E[X^2] - (E[X])^2\\
        &= np[(n-1)p + 1] = (np)^2\\
        &= np(1-p)
    \end{split}
\end{equation*}
To reiterate, we have \[E[X] = np\] \[Var(X) = np(1-p)\]
\subsubsection{Proposition}
If $X$ is a binomial r.v. with parameters $n,p$ where $0<p<1$, then as $k$ goes from 0 to $n$, $P\{X = k\}$ first increase monotonically and then decreases monotonically, reaching its largest value when $k$ is the largest integer less than or equal $(n+1)p$ (Normal Distribution)
\subsection{Binomial Distribution Function}
\[P\{X = k + 1\} = \frac{p}{1-p}\frac{n-k}{k+1}P\{X = k\}\]
\section{The Poisson Random Variable}
\begin{definition}[Poisson R.V.]
    A random variable X that takes on one of the values 0,1,2,\dots is said to be a \textbf{Poisson r.v.} with parameter $\lambda$ if, for some $\lambda > 0$, \[p(i) = P\{X = i\} = e^{-\lambda}\frac{\lambda^i}{i!}\qquad i = 0,1,2,\dots\]
    with $p(i)$ being the probability mass function.  A Poisson r,v, approximates a binomial r.v. with parameters n and p when n is large, p is small, and $\lambda = np$.
\end{definition}
\subsection*{Example}
Suppose that the number of typographical errors on a single page of this book have a Poisson distribution with parameter $\lambda = \frac{1}{2}$. Calculate the probability that there is at least one error of this page. \\
Letting $X$ denote the number of errors on this page, we have \[P\{X\geq 1\} = 1 - P\{X = 0\} = 1 - e^{-1\frac{1}{2}}\approx .393\]

Since the Poisson r.v. approximates a binomial r.v., we can say that the expected value is $np = \lambda$ and variance $np(1-p) = \lambda (1-p)\approx\lambda \text{ (since p is small)}$.\\
\subsection{Poisson Random Variable for points in time}
Let us suppose that the events are indeed occurring at certain (random) points of time, and let us assume that for some positive constant $\lambda$, the following assumptions are true:
\begin{enumerate}
    \item The probability that exactly 1 event occurs in a given interval of length $h$ is equal to $\lambda h + o(h)$,where $o(h)$ stands for any function $f(h)$ for which $\lim_{h\rightarrow 0} f(h)/h = 0$.
    \item The probability that 2 or more events occur in an interval of length $h$ is equal to $o(h)$.
    \item For any integers $n, j_1, j_2, \dots, j_n$ any any set of n non overlapping intervals, if we defined $E_i$ to be the event that exactly $j_i$ of the events under consideration occur in the $i$th of these intervals, then $E_1,E_2,\dots, E_n$ are independent.
\end{enumerate}
\subsection*{Example}
Suppose that earthquakes occur in the western portion of the United States in accordance with assumptions 1,2,3 with $\lambda = 2$ and with 1 week as the unit of time. (Earthquakes occur in accordance with the three assumptions at a rate of 2 per week)
\begin{enumerate}[a. ]
    \item Find the probability that at least 3 earthquakes occur during the next 2 weeks. 
    \item Find the probability distribution of the time, starting from now, until the next earthquake. 
\end{enumerate}
\subsubsection*{Solution}
\begin{enumerate}[a. ]
    \item We have \begin{equation*}
        \begin{split}
            P\{N(2)\geq 3\} &= 1 - P\{N(2) = 0\} - P\{N(2) = 1\} - P\{N(2) = 2\}\\
            &= 1- e^{-4} - 4e^{-4} - \frac{4^2}{2}e^{-4}\\
            &= 1 - 13e^{-4}
        \end{split}
    \end{equation*}
    \item Let $X$ denote the time in weeks until the next earthquake. Because $X$ will be greater than $t$ if and only if no events occur within the next $t$ units of time, we have \[P\{X > t\} = P\{N(t) = 0\} = e^{-\lambda t}\]
    so the probability distribution function $F$ of the r.v. $X$ is given by 
    \begin{equation*}
        \begin{split}
            F(t) = P\{X\leq t\} &= 1- \{P > t\} = 1 - e^{-\lambda t}\\
            &= 1 - e^{-2t}
        \end{split}
    \end{equation*}
\end{enumerate}
\subsection{Poisson Distribution Function}
\[\frac{P\{X = i + 1\}}{P\{X = i\}} = \frac{e^{-\lambda}\lambda^{i+1}/(i+1)!}{e^{-\lambda}\lambda^i/i!} = \frac{\lambda}{i+1}\]
\section{Geometric Random Variable}
Suppose that independent trials, each having a probability $p, 0 < p < 1$, of being a success, are performed until a success occurs. If we let $X$ equal the number of trials required, then \[P\{X = n\} = (1-p)^{n-1}p\qquad n= 1,2,\dots\]
Any r.v. $X$ whose pmf is said to be a geometric random variable with parameter $p$
\subsection*{Example}
An urn contains N white and M black balls. Balls are randomly selected, one at a time, until a black one is obtained. If we assume that each ball selected is replaced before the next one is drawn, what is the probability that
\begin{enumerate}[a. ]
    \item exactly $n$ draws are needed?
    \item at least $k$ draws are needed?
\end{enumerate}
\subsubsection*{Solution}
If we let $X$ denote the number of draws needed to select a black ball, then $X$ satisfies the previous equation with $p = \frac{M}{M+N}$. Then, 
\begin{enumerate}[a. ]
    \item \[P\{X = n\} = \left(\frac{N}{M + N}\right)^{n-1} \frac{M}{M+N} = \frac{MN^{n-1}}{(M+N)^n}\] \[P\{X\geq k\} = \frac{M}{M+N}\sum^\infty_{n=k} \left(\frac{N}{M+N}\right)^{n-1}\]
    \item \[ = \left.\left(\frac{M}{M+N}\right)\left(\frac{N}{M+N}\right)^{k-1}\middle/ \left[1 - \frac{N}{M+N}\right]\right.\]  \[ = \left(\frac{N}{N+N}\right)^{k-1}\]
\end{enumerate}
An easier solution for b) is to realize that at least $k$ trials are necessary to obtain a success that is equal to the probability that the first $k-1$ trials are failures. Then we have, 
\[P\{X\geq k\} = (1-p)^{k-1}\]
\[E[X] = \frac{1}{p}\qquad Var(X) = \frac{1- p}{p^2}\]
\section{Expected Value of Sums of Random Variables}
For a random variable $X$, let $X(s)$ denote the value of $X$ when $s\in S$ is the outcome for the experiment. Now if $X$ and $Y$ are both random variables, then \textbf{so is their sum}. That is $Z = X + Y$ is also a r.v. Moreover, \[Z(s) = X(s) + Y(s)\]
\textbf{Proposition}
\[E[X] = \sum_{s\in S} X(s)p(s)\]
\begin{corollary}
For random variables $X_1, \dots, X_n$, \[E\left[\sum^n_{i=1} X_i\right] = \sum^n_{i=1} E[X_i]\]
\section{Properties of the Cumulative Distribution Function (CDF)}
\begin{enumerate}
    \item $F$ is a non decreasing function; that is, if $a < b$, then $F(a)\leq F(b)$.
    \item $\lim_{b\rightarrow\infty} F(b) = 1$
    \item $\lim_{b\rightarrow -\infty} F(b) = 0$
    \item F is right continuous. That is, for any $b$ any decreasing sequence $b_n, n\geq 1$, that converges to $b$. \[\lim_{n\rightarrow\infty} F(b_n) = F(b)\]
\end{enumerate}
\end{corollary}